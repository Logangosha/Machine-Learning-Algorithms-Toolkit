{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1b28cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Step 1: Generate synthetic high-dimensional data\n",
    "# -----------------------------------------\n",
    "\n",
    "# Create a dataset with:\n",
    "# - 300 samples (rows)\n",
    "# - 6 features (columns)# 🤖 DBSCAN Clustering on a Human-Like Synthetic Dataset\n",
    "# This example creates fake \"people\" with features like height, weight, age, etc.,\n",
    "# and clusters them into lifestyle groups using DBSCAN.\n",
    "\n",
    "# -----------------------------------------\n",
    "# 📦 Step 1: Import Required Libraries\n",
    "# -----------------------------------------\n",
    "\n",
    "import numpy as np  # 🔢 NumPy is the standard library for numerical operations in Python\n",
    "from sklearn.datasets import make_classification  # 🔬 For generating synthetic, labeled datasets\n",
    "from sklearn.cluster import DBSCAN  # 📊 The core DBSCAN algorithm from scikit-learn\n",
    "from dbscan import DBSCANCluster  # 🧩 Your custom wrapper for sklearn's DBSCAN (defined elsewhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "569ca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6)\n",
      "[[-1.70020487 -1.47399908 -2.53505051  0.52830303  1.25343421  2.55157169]\n",
      " [ 0.68656601  0.03577646 -2.02979925 -2.21322735 -1.11958064  1.62908285]\n",
      " [-1.45071192 -0.56028716 -0.2855498  -0.31675333  0.78710345  0.10678175]\n",
      " [-1.3277159   0.43763299 -1.30427838 -0.01581971 -0.55473821  0.99606491]\n",
      " [-0.43253226 -1.30982893 -0.56996467  1.3270781  -1.0736925   0.04971871]]\n",
      "[2 1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 🧠 Step 2: Generate Raw Synthetic Data\n",
    "# -----------------------------------------\n",
    "\n",
    "# make_classification returns:\n",
    "# - X_raw: features (NumPy array), shape = [n_samples, n_features]\n",
    "# - _: labels (unused here, so we assign to _ which is Python’s \"I don’t care\" variable)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 📊 What is X_raw and where do its values come from?\n",
    "# -----------------------------------------\n",
    "\n",
    "# X_raw is the raw feature matrix generated by sklearn's make_classification().\n",
    "# It contains 300 samples (rows), each with 6 features (columns), generated to simulate\n",
    "# real-world classification data. However, we use it for clustering.\n",
    "\n",
    "# 🔬 The values inside X_raw are not purely random. They're generated using\n",
    "# multivariate Gaussian (normal) distributions with structured patterns.\n",
    "\n",
    "# 🧠 Of the 6 total features:\n",
    "# - 4 are \"informative\": they contain real signals that separate the (synthetic) classes.\n",
    "# - 0 are \"redundant\": we didn’t ask for any features that are linear combinations of others.\n",
    "# - The remaining 2 are essentially noise (still structured, but not useful for separation).\n",
    "\n",
    "# 📈 Range of values:\n",
    "# - Because the features are sampled from normal distributions,\n",
    "#   most values fall within the range of approximately -3.0 to +3.0.\n",
    "# - These are not hard limits, but about 99.7% of the values will stay in this range.\n",
    "# - Each feature may have a different mean and spread (std), depending on the class separation.\n",
    "\n",
    "# ⚠️ Important: These values are not yet scaled to real-world interpretable units.\n",
    "# That's why we later rescale them using np.interp() to represent things like:\n",
    "#   - height (cm) → [150, 200]\n",
    "#   - weight (kg) → [50, 100]\n",
    "#   - age (years) → [18, 70]\n",
    "#   - income (k/year) → [20, 120]\n",
    "#   - etc.\n",
    "\n",
    "# 💡 Summary:\n",
    "# - X_raw is structured fake data made to mimic real class-based clustering behavior.\n",
    "# - It is ideal for testing clustering algorithms like DBSCAN.\n",
    "# - Scaling is required to make the features interpretable and comparable in distance-based methods.\n",
    "\n",
    "\n",
    "X_raw, _ = make_classification(\n",
    "    n_samples=300,         # 🧪 300 rows = 300 fake \"people\"\n",
    "    n_features=6,          # 📐 Each person has 6 features (e.g. height, weight, etc.)\n",
    "    n_informative=4,       # ✅ 4 features actually contribute to class separation\n",
    "    n_redundant=0,         # 🔁 No duplicate or combined features\n",
    "    n_clusters_per_class=1,# 🧩 Each class is generated as one cluster in feature space\n",
    "    n_classes=3,           # 🎯 Total of 3 lifestyle classes (we ignore these for clustering)\n",
    "    random_state=42        # 🔁 Fixes randomness for reproducibility (same data every run)\n",
    ")\n",
    "\n",
    "print(X_raw.shape)  # 🖨️ Print shape to confirm we have 300 samples with 6 feature\n",
    "print(X_raw[:5])  # 🖨️ Print first 5 rows to see the generated data\n",
    "print(_[:5])  # 🖨️ Print first 5 labels (not used in clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b90e19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# ⚙️ Step 3: Rescale Features to Human-Like Ranges\n",
    "# -----------------------------------------\n",
    "\n",
    "# 🤓 We want each feature to represent something real-world and interpretable.\n",
    "# We use NumPy’s `np.interp()` to rescale each column (feature) from its raw range to a defined real-world range.\n",
    "\n",
    "# Initialize an empty array to hold rescaled data (same shape as X_raw)\n",
    "X = np.empty_like(X_raw)  # ❗ np.empty_like creates an uninitialized array with same shape/type as X_raw\n",
    "\n",
    "# 🔁 Rescale each feature column individually:\n",
    "\n",
    "# Feature 0: Height in cm → map to range [150, 200]\n",
    "X[:, 0] = np.interp(\n",
    "    X_raw[:, 0],                          # Input data (all rows, column 0)\n",
    "    (X_raw[:, 0].min(), X_raw[:, 0].max()), # Old range\n",
    "    [150, 200]                            # New desired range\n",
    ")\n",
    "\n",
    "# Feature 1: Weight in kg → [50, 100]\n",
    "X[:, 1] = np.interp(X_raw[:, 1], (X_raw[:, 1].min(), X_raw[:, 1].max()), [50, 100])\n",
    "\n",
    "# Feature 2: Age in years → [18, 70]\n",
    "X[:, 2] = np.interp(X_raw[:, 2], (X_raw[:, 2].min(), X_raw[:, 2].max()), [18, 70])\n",
    "\n",
    "# Feature 3: Income in thousands/year → [20, 120]\n",
    "X[:, 3] = np.interp(X_raw[:, 3], (X_raw[:, 3].min(), X_raw[:, 3].max()), [20, 120])\n",
    "\n",
    "# Feature 4: Spending score (1–100)\n",
    "X[:, 4] = np.interp(X_raw[:, 4], (X_raw[:, 4].min(), X_raw[:, 4].max()), [1, 100])\n",
    "\n",
    "# Feature 5: Weekly exercise hours → [0, 12]\n",
    "X[:, 5] = np.interp(X_raw[:, 5], (X_raw[:, 5].min(), X_raw[:, 5].max()), [0, 12])\n",
    "\n",
    "# 🔍 At this point, each row in X represents a \"person\" with realistic features:\n",
    "# Example:\n",
    "#   [176.2 cm, 82.5 kg, 29 yrs, $55k/year, 75 score, 5.5 hrs/week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37a526d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 🧪 Step 4: Apply DBSCAN Clustering\n",
    "# -----------------------------------------\n",
    "\n",
    "# eps (epsilon): max distance between two samples for one to be considered in the neighborhood of the other\n",
    "# min_samples: minimum number of points to form a dense region (i.e., a cluster)\n",
    "\n",
    "clusterer = DBSCANCluster(eps=25, min_samples=5)\n",
    "\n",
    "# Fit the DBSCAN model and assign cluster labels\n",
    "labels = clusterer.fit_predict(X)  # returns 1D array like: [0, 0, 1, 1, -1, ...]\n",
    "\n",
    "# 💡 Note: \n",
    "# - Each number represents a cluster.\n",
    "# - -1 means DBSCAN considered that point to be noise (outlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cc5a3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏷️ Unique clusters found: {np.int64(0), np.int64(-1)}\n",
      "❌ Noise points (label = -1): 1\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 📤 Step 5: Print Clustering Results\n",
    "# -----------------------------------------\n",
    "\n",
    "# Get the set of unique cluster labels (excluding duplicates)\n",
    "print(\"🏷️ Unique clusters found:\", set(labels))\n",
    "# Example output: {0, 1, -1}\n",
    "\n",
    "# Count how many points were labeled as noise (-1)\n",
    "print(\"❌ Noise points (label = -1):\", list(labels).count(-1))\n",
    "\n",
    "# 👉 You can use these labels to:\n",
    "# - Group people by lifestyle\n",
    "# - Flag outliers for further inspection\n",
    "\n",
    "# - 3 classes\n",
    "# - 4 informative features (the rest are noise)\n",
    "X, _ = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
