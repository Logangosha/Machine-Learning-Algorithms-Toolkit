{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba3a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 SVM Example: Classifying Synthetic Data\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 📦 Import libraries\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from svm import SVMModel  # 🧠 Our custom SVM wrapper (uses scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 🎲 Step 1: Generate synthetic classification data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ❓ Why: This step creates fake data to simulate a real-world binary classification task.\n",
    "# It's useful for experimenting with algorithms without needing a real dataset.\n",
    "\n",
    "# 💡 Fun fact: Synthetic data is widely used in ML to test models before applying them to real problems!\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ✅ Step A: Set the random seed for reproducibility\n",
    "# ------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "# Setting a seed makes sure we get the same \"random\" results each time we run the code.\n",
    "# Great for debugging or sharing results with others.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧊 Step B: Generate data points for Class 0\n",
    "# ------------------------------------------------------------\n",
    "# The covariance matrix controls how data points are spread and tilted in space.\n",
    "# The diagonal values represent variance along the x and y axes (how wide the data spreads),\n",
    "# while the off-diagonal values represent covariance (how much x and y move together).\n",
    "# A positive covariance tilts the data upward (x↑ → y↑), a negative one tilts it downward (x↑ → y↓),\n",
    "# and zero means no directional relationship. Adjusting this matrix changes the shape and orientation\n",
    "# of the data cloud, making it a powerful tool for simulating realistic patterns in synthetic datasets.\n",
    "\n",
    "class0 = np.random.multivariate_normal(\n",
    "    mean=[2, 2],                # 📍 Center of the class 0 cluster (x=2, y=2)\n",
    "    cov=[[1, 0.5], [0.5, 1]],   # 🔄 Covariance matrix (controls shape/orientation of the spread)\n",
    "    size=50                     # 🎯 Number of data points to generate\n",
    ")\n",
    "# This creates 50 points clustered around (2,2) with some spread/tilt.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🔥 Step C: Generate data points for Class 1\n",
    "# ------------------------------------------------------------\n",
    "class1 = np.random.multivariate_normal(\n",
    "    mean=[5, 5],                # 📍 Center of the class 1 cluster (x=5, y=5)\n",
    "    cov=[[1, -0.5], [-0.5, 1]], # 🔄 Covariance gives it a different spread/direction than class0\n",
    "    size=50                     # 🎯 Also generating 50 points here\n",
    ")\n",
    "# Now we have a second cluster of 50 points around (5,5), forming our second class.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧩 Step D: Combine the feature data from both classes\n",
    "# ------------------------------------------------------------\n",
    "X = np.vstack((class0, class1))\n",
    "# 🧱 `vstack` vertically stacks the two arrays (50 + 50 = 100 rows)\n",
    "# Final shape of X: (100, 2) — 100 samples, each with 2 features (x and y coordinates)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🏷️ Step E: Create the labels for the classes\n",
    "# ------------------------------------------------------------\n",
    "y = np.array([0]*50 + [1]*50)\n",
    "# 🧠 First 50 labels are 0 (class0), next 50 are 1 (class1)\n",
    "# Final shape of y: (100,) — 100 labels total\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 📊 Step F: Visualize the synthetic data\n",
    "combined_dataset = np.column_stack((y, X))\n",
    "print(\"Combined dataset: \\n\", combined_dataset)\n",
    "\n",
    "# ✅ Now, X and y together form a labeled dataset you can use for training/testing classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8000812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ✂️ Step 2: Train/test split\n",
    "# ------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🧠 Step 3: Train SVM model\n",
    "# ------------------------------------------------------------\n",
    "# The SVM kernel defines how it draws the decision boundary between classes.\n",
    "# 🔹 \"linear\" — Best for data that’s linearly separable (straight line boundary).\n",
    "# 🔹 \"rbf\" (Radial Basis Function) — Handles complex, curvy boundaries. Great default!\n",
    "# 🔹 \"poly\" — Uses polynomial functions (quadratic, cubic, etc.). Good for medium complexity.\n",
    "# 🔹 \"sigmoid\" — Mimics a neural net activation; rarely used in practice.\n",
    "# 💡 Tip: Try different kernels when accuracy is low — each one sees the data differently!\n",
    "model = SVMModel(kernel=\"linear\")  \n",
    "model.train(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2023aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 🔍 Step 4: Make predictions and evaluate\n",
    "# ------------------------------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 📈 Step 5: Visualize decision boundary\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Visualizes the decision boundary of a trained SVM model on 2D data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A trained classifier with a .predict() method (e.g., SVM).\n",
    "    - X: Feature matrix of shape (n_samples, 2), where each row is [x1, x2].\n",
    "    - y: Label array of shape (n_samples,), with class labels (e.g., 0 or 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 🧱 Step 1: Create mesh grid\n",
    "    # -------------------------\n",
    "\n",
    "    # This step builds a \"canvas\" of 2D points that we'll color based on the model's predictions.\n",
    "\n",
    "    # Set the step size for the meshgrid.\n",
    "    # Smaller = smoother boundary, but more computation.\n",
    "    h = 0.02\n",
    "\n",
    "    # Extract x-axis range (first feature) from the dataset:\n",
    "    # X[:, 0] → All rows of column 0 (feature 1)\n",
    "    x_feature_column = X[:, 0]\n",
    "    x_min = x_feature_column.min() - 1  # Minimum x value minus padding\n",
    "    x_max = x_feature_column.max() + 1  # Maximum x value plus padding\n",
    "\n",
    "    # Extract y-axis range (second feature) from the dataset:\n",
    "    # X[:, 1] → All rows of column 1 (feature 2)\n",
    "    y_feature_column = X[:, 1]\n",
    "    y_min = y_feature_column.min() - 1  # Minimum y value minus padding\n",
    "    y_max = y_feature_column.max() + 1  # Maximum y value plus padding\n",
    "\n",
    "    # Generate evenly spaced values across the x and y ranges\n",
    "    x_values = np.arange(x_min, x_max, h)\n",
    "    y_values = np.arange(y_min, y_max, h)\n",
    "\n",
    "    # Create a meshgrid from these x and y values:\n",
    "    #   xx and yy are 2D arrays that together represent all (x, y) coordinate pairs\n",
    "    #   This will create a grid of points we can classify\n",
    "    xx, yy = np.meshgrid(x_values, y_values)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 🤖 Step 2: Predict labels for each point in the mesh grid\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    # Flatten xx and yy and stack them column-wise using np.c_ to create input feature vectors\n",
    "    # np.c_ → Concatenates arrays column-wise (axis=1)\n",
    "    # Example:\n",
    "    #   xx.ravel() = [x1, x2, x3, ...]\n",
    "    #   yy.ravel() = [y1, y2, y3, ...]\n",
    "    #   np.c_[xx.ravel(), yy.ravel()] = [[x1, y1], [x2, y2], [x3, y3], ...]\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Shape: (num_grid_points, 2)\n",
    "\n",
    "    # Use the model to predict class labels (0 or 1) for each grid point\n",
    "    Z = model.predict(grid_points)  # Output shape: (num_grid_points,)\n",
    "\n",
    "    # Reshape the prediction array to match the meshgrid shape (2D)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 🎨 Step 3: Plot the decision boundary + points\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Start a new figure with a defined size\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Draw filled contours based on predictions (Z)\n",
    "    # This colors the background based on the predicted class at each (x, y) point\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "\n",
    "    # Plot the actual data points\n",
    "    # X[:, 0] → x-coordinates\n",
    "    # X[:, 1] → y-coordinates\n",
    "    # y       → color for each point based on its class label\n",
    "    plt.scatter(\n",
    "        X[:, 0],           # Feature 1 (x-axis)\n",
    "        X[:, 1],           # Feature 2 (y-axis)\n",
    "        c=y,               # Color points by their label (0 or 1)\n",
    "        cmap=plt.cm.coolwarm,\n",
    "        edgecolors=\"k\"     # Outline each point with a black border\n",
    "    )\n",
    "\n",
    "    # Add plot details\n",
    "    plt.title(\"SVM Decision Boundary\")    # Title\n",
    "    plt.xlabel(\"Feature 1\")               # x-axis label\n",
    "    plt.ylabel(\"Feature 2\")               # y-axis label\n",
    "    plt.grid(True)                        # Show grid\n",
    "    plt.tight_layout()                    # Fit layout cleanly\n",
    "\n",
    "    # Display the final plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 🧪 Call the function with your trained model and data\n",
    "plot_decision_boundary(model, X, y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
